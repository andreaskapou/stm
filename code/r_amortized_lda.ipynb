{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac48d68a-2a3f-4229-968c-8b54b318bafb",
   "metadata": {},
   "source": [
    "# Amortized LDA implementation\n",
    "\n",
    "## Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6785b408-c119-4dc8-91c8-1506be70dec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import logging\n",
    "import pyro\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from zzz_utils import *\n",
    "from amortized_lda import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(format=\"%(relativeCreated) 9d %(message)s\", level=logging.INFO)\n",
    "\n",
    "pyro.clear_param_store()\n",
    "pyro.set_rng_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e89bcb-d247-4508-a9b6-f185cbf9053f",
   "metadata": {},
   "source": [
    "## Amortized LDA graphical model\n",
    "\n",
    "First we simulate a toy dataset to render the Pyro models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3c02c3-1954-45c8-98ce-f20014ba4fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nTopics = 3    # Topics\n",
    "nCells = 50    # Cells\n",
    "nRegions = 100 # Regions\n",
    "N = [30] * nCells # Cells size\n",
    "\n",
    "# Simulate data\n",
    "obj = simulate_lda_dataset(nTopics = nTopics, nCells = nCells, \n",
    "                           nRegions = nRegions, N = N, \n",
    "                           a = [1] * nTopics, b = [1] * nRegions)\n",
    "# transpose so it matches Pyro's input\n",
    "D = torch.from_numpy(obj['D'].transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21da882b-fcba-4d1e-9da3-1d0544a99a0c",
   "metadata": {},
   "source": [
    "Below we define the LDA model with Pyro. Note that data D is a matrix of nCounts x nCells, and here we assume that nCounts is the same across cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc31cace-405c-49bf-812d-06082e97f3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "pyro.render_model(amortized_lda_model, model_args=(D, nTopics, nRegions), \n",
    "                  render_distributions=True, render_params=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f03290a-f215-480f-9cfb-b15d2e3e7866",
   "metadata": {},
   "source": [
    "##Â Amortized LDA guide (variational approximation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa0e4b-05ee-4ba1-bf10-3e134a13b098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NN predictor\n",
    "layer_sizes = \"100-100\"\n",
    "pred = nn_predictor(nTopics, nRegions, layer_sizes)\n",
    "guide = functools.partial(amortized_lda_guide, pred)\n",
    "pyro.render_model(guide, model_args=(D, nTopics, nRegions, 20), \n",
    "                  render_distributions=True, render_params=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c9e95a-c118-4bf6-a3ba-270a54880d20",
   "metadata": {},
   "source": [
    "# Testing variational inference\n",
    "\n",
    "## Simulate data\n",
    "We use simulated data from the LDA model to test the amortized LDA inference performance. \n",
    "I.e. how close are inferred values to true values used to simulate the data. \n",
    "\n",
    "__Note__ there is the known identifiability issue of mixture and mixed-membership models, however we still would expect cell assignments to be consistent with the simulated data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb8dc76-e515-4181-b6d9-c6e295a01dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data\n",
    "nTopics = 2    # Topics\n",
    "nCells = 1000    # Cells\n",
    "nRegions = 300 # Regions\n",
    "N = [100] * nCells # Cells size\n",
    "a = [1/5] * nTopics\n",
    "b = [1/10] * nRegions\n",
    "# Simulate data\n",
    "obj = simulate_lda_dataset(nTopics = nTopics, nCells = nCells, \n",
    "                           nRegions = nRegions, N = N, \n",
    "                           a = a, b = b)\n",
    "# transpose so it matches Pyro's input\n",
    "D = torch.from_numpy(obj['D'].transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df54636-8d85-43e7-bd80-6045883ae78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.shape # simulated data dims nCounts x nCells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5934079-0a49-47de-9df0-f87853a65359",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj['theta_true'][1:10, ] # first 10 cells prob assignments to each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da83aef3-9c5d-46b1-8a0f-c2252d2c7f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj['phi_true'][:, 1:10].transpose() # first 10 region-topic probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba7715d-1ca8-4420-b3e7-89299a4846a5",
   "metadata": {},
   "source": [
    "## Fit AmortizedLDA \n",
    "\n",
    "To perform inference for Amortized LDA with use ClippedAdam to optimize a \n",
    "__trace implementation of ELBO-based SVI__ ('TraceEnum_ELBO'), which supports exhaustive enumeration \n",
    "over discrete sample sites, in our case latent topic assignment __z__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dec6bb-e835-46af-b771-0ee335794ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "obj = fit_amortized_lda(D = D, nTopics = nTopics, nRegions = nRegions, nSteps = 3000, batch_size=64, lr = 0.01, seed = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf54a2a-aff1-4abf-904f-00d4f4945f6e",
   "metadata": {},
   "source": [
    "Here we plot the ELBO loss during optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b76d78-8566-4264-8c14-5b53ffc78b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ELBO losses\n",
    "losses = obj['losses']\n",
    "\n",
    "plt.figure(figsize=(5, 2))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"SVI step\")\n",
    "plt.ylabel(\"ELBO loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49441ee1-eb71-47ce-b51f-de99d9217715",
   "metadata": {},
   "source": [
    "## Assessing inferred parameters\n",
    "\n",
    "Below we show estimates of the inferred model parameters. To show this, here I am just taking a sample from the \n",
    "posterior fit (i.e. calling the guide with the optimized set of variational parameters). Surely this is not the optimal way to summarise the posterior fit. \n",
    "\n",
    "__However__, if I take multiple samples from the posterior and subseqently summarise the posterior samples (e.g. by median), due to label switching that occurs when sampling \n",
    "$\\theta \\sim Dir(\\alpha)$, the posterior mode will be useless. \n",
    "\n",
    "__TODO__ \n",
    "\n",
    "1. Define a better way to summarize the posterior distribution from posterior samples. E.g. by fixing the label switching problem, post-hoc after sampling from the posterior (similar approach to mixture models).\n",
    "2. Make posterior predictive checks.\n",
    "3. Need to understand Pyro's `poutine`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10067ab5-b768-4ac6-8501-6b0ca42bf7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the fitted guide object, from which we will sample from.\n",
    "guide = obj['guide']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa7c2f3-a4b3-44a9-b8c3-6d33b5376870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single sample from the guide\n",
    "post_sample = guide(D = D, nTopics = nTopics, nRegions = nRegions, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3b4081-df93-426e-8da4-df43d8a7e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_sample['alpha'] # posterior alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c93e8ca-6a1c-4606-8456-1ad213a10bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_sample['phi'][:, 1:10].detach().numpy().transpose() # posterior phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af995a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_sample['phi'][:, 1:10].detach().numpy().transpose() # posterior phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defbd45d-3331-48e8-b3a7-b0543222fe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.param(\"b_vi\")[:, 1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c53e02-bbcf-4f51-9b73-d598dc12b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract optimized values of variational parameters\n",
    "\n",
    "#for name, value in pyro.get_param_store().items():\n",
    "#    print(name, pyro.param(name).data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8b73aa-5b1f-43c6-ab51-b5972e406a25",
   "metadata": {},
   "source": [
    "## Testing (ignore for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa034d00-790d-41c6-bebf-19598b8364df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "obj['D_str'][2]\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=1.0, min_df=0, stop_words=None)\n",
    "docs = torch.from_numpy(vectorizer.fit_transform(obj['D_str']).toarray())\n",
    "\n",
    "vocab = pd.DataFrame(columns=['word', 'index'])\n",
    "vocab['word'] = vectorizer.get_feature_names()\n",
    "vocab['index'] = vocab.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a213f486-9a3a-4a7f-84ad-2d0d32dac7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e2491b-5d85-4629-803a-c23902bd69f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj['D_freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48f351a-7658-40e3-bf87-ae9907524705",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj['D_tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431e36ee-8c48-4184-b754-0b6f1446edef",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj['D_freq']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
